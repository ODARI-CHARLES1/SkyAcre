{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc670fd3",
   "metadata": {},
   "source": [
    "# SkyAcre — Colab Training Pipeline\n",
    "This notebook contains a GPU-ready preprocessing and 5-fold cross-validation training pipeline.\n",
    "Sections: Imports & GPU check → Data loading → Preprocessing → Model builder → 5-fold CV → Final training & saving.\n",
    "Instructions: Mount Google Drive (if you want to save models), set `DATA_DIR`, then run cells top-to-bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0774b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: ensure packages (Colab usually has tensorflow preinstalled)\n",
    "!pip install -q scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and GPU check\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "def check_gpu():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print('GPUs found:', gpus)\n",
    "    if gpus:\n",
    "        try:\n",
    "            for g in gpus:\n",
    "                tf.config.experimental.set_memory_growth(g, True)\n",
    "            print('GPU available and memory growth set')\n",
    "        except Exception as e:\n",
    "            print('Error setting memory growth:', e)\n",
    "    else:\n",
    "        print('No GPU available; training will use CPU')\n",
    "\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175100ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading helpers\n",
    "def load_data_from_npy(data_dir):\n",
    "    paths = {\n",
    "        'X_train': os.path.join(data_dir, 'X_train.npy'),\n",
    "        'y_train': os.path.join(data_dir, 'y_train.npy'),\n",
    "        'X_val': os.path.join(data_dir, 'X_val.npy'),\n",
    "        'y_val': os.path.join(data_dir, 'y_val.npy'),\n",
    "        'X_test': os.path.join(data_dir, 'X_test.npy'),\n",
    "        'y_test': os.path.join(data_dir, 'y_test.npy')\n",
    "    }\n",
    "    data = {}\n",
    "    for k,p in paths.items():\n",
    "        if os.path.exists(p):\n",
    "            data[k] = np.load(p)\n",
    "            print(f'Loaded {k} from {p} with shape {data[k].shape}')\n",
    "        else:\n",
    "            data[k] = None\n",
    "    return data\n",
    "\n",
    "def load_csv(data_path, features=None, target_col=None):\n",
    "    df = pd.read_csv(data_path)\n",
    "    if target_col is None:\n",
    "        raise ValueError('target_col must be provided for CSV loading')\n",
    "    if features is None:\n",
    "        X = df.drop(columns=[target_col]).values\n",
    "    else:\n",
    "        X = df[features].values\n",
    "    y = df[target_col].values\n",
    "    print('Loaded CSV', data_path, 'X shape', X.shape, 'y shape', y.shape)\n",
    "    return X, y\n",
    "\n",
    "def detect_problem_type(y):\n",
    "    if y is None:\n",
    "        return None\n",
    "    if np.issubdtype(y.dtype, np.integer) or np.issubdtype(y.dtype, np.bool_):\n",
    "        n_classes = len(np.unique(y))\n",
    "        if n_classes <= 50:\n",
    "            return 'classification'\n",
    "    if np.issubdtype(y.dtype, np.floating):\n",
    "        return 'regression'\n",
    "    return 'classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7589a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and dataset utility\n",
    "def get_preprocessing_fn(X_train, X_val=None):\n",
    "    if X_train.ndim == 4:\n",
    "        def preprocess_train(x):\n",
    "            return x.astype('float32') / 255.0\n",
    "        return preprocess_train, preprocess_train\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train.reshape((X_train.shape[0], -1)))\n",
    "        def preprocess_train(x):\n",
    "            shp = x.shape\n",
    "            flat = x.reshape((shp[0], -1))\n",
    "            scaled = scaler.transform(flat)\n",
    "            return scaled.reshape(shp)\n",
    "        def preprocess_infer(x):\n",
    "            shp = x.shape\n",
    "            flat = x.reshape((shp[0], -1))\n",
    "            scaled = scaler.transform(flat)\n",
    "            return scaled.reshape(shp)\n",
    "        return preprocess_train, preprocess_infer\n",
    "\n",
    "def make_dataset(X, y=None, batch_size=32, shuffle=False):\n",
    "    if y is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X))\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model builder (MLP for tabular, small CNN for images)\n",
    "def build_model(input_shape, problem_type='classification', n_classes=None, hidden_units=[128,64], dropout=0.3):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    if len(input_shape) == 1:\n",
    "        for units in hidden_units:\n",
    "            x = layers.Dense(units, activation='relu')(x)\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "        if problem_type == 'classification':\n",
    "            if n_classes is None or n_classes <= 2:\n",
    "                outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "            else:\n",
    "                outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "        else:\n",
    "            outputs = layers.Dense(1, activation='linear')(x)\n",
    "    elif len(input_shape) == 3:\n",
    "        x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPool2D()(x)\n",
    "        x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPool2D()(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        if problem_type == 'classification':\n",
    "            if n_classes is None or n_classes <= 2:\n",
    "                outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "            else:\n",
    "                outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "        else:\n",
    "            outputs = layers.Dense(1, activation='linear')(x)\n",
    "    else:\n",
    "        x = layers.Flatten()(x)\n",
    "        for units in hidden_units:\n",
    "            x = layers.Dense(units, activation='relu')(x)\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "        if problem_type == 'classification':\n",
    "            if n_classes is None or n_classes <= 2:\n",
    "                outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "            else:\n",
    "                outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "        else:\n",
    "            outputs = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    if problem_type == 'classification':\n",
    "        if n_classes is None or n_classes <= 2:\n",
    "            loss = 'binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "        else:\n",
    "            loss = 'sparse_categorical_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "    else:\n",
    "        loss = 'mse'\n",
    "        metrics = ['mae']\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1bdf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation training loop\n",
    "def run_k_fold_cv(X, y, problem_type=None, n_splits=5, batch_size=64, epochs=50, model_builder=build_model, save_dir='/content/drive/MyDrive/models'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    if problem_type is None:\n",
    "        problem_type = detect_problem_type(y)\n",
    "    print('Detected problem type:', problem_type)\n",
    "\n",
    "    if problem_type == 'classification':\n",
    "        n_classes = len(np.unique(y))\n",
    "    else:\n",
    "        n_classes = None\n",
    "\n",
    "    if problem_type == 'classification' and len(np.unique(y)) > 1:\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = kf.split(X, y)\n",
    "    else:\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = kf.split(X)\n",
    "\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits, 1):\n",
    "        print(f'\\n===== Fold {fold}/{n_splits} =====')\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        preprocess_train, preprocess_infer = get_preprocessing_fn(X_train)\n",
    "        X_train_pre = preprocess_train(X_train)\n",
    "        X_val_pre = preprocess_infer(X_val)\n",
    "\n",
    "        if X_train_pre.ndim > 2 and X_train_pre.shape[-1] in (1,3):\n",
    "            input_shape = X_train_pre.shape[1:]\n",
    "        elif X_train_pre.ndim == 2:\n",
    "            input_shape = (X_train_pre.shape[1],)\n",
    "        else:\n",
    "            input_shape = X_train_pre.shape[1:]\n",
    "\n",
    "        model = model_builder(input_shape=input_shape, problem_type=problem_type, n_classes=n_classes)\n",
    "        model.summary()\n",
    "\n",
    "        cb = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "        ]\n",
    "\n",
    "        train_ds = make_dataset(X_train_pre, y_train, batch_size=batch_size, shuffle=True)\n",
    "        val_ds = make_dataset(X_val_pre, y_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cb, verbose=2)\n",
    "\n",
    "        y_pred_prob = model.predict(val_ds)\n",
    "        if problem_type == 'classification':\n",
    "            if n_classes is None or n_classes <= 2:\n",
    "                y_pred = (y_pred_prob.ravel() > 0.5).astype(int)\n",
    "            else:\n",
    "                y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            prec = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "            rec = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "            metrics = {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "        else:\n",
    "            y_pred = y_pred_prob.ravel()\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            metrics = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "        print(f'Fold {fold} metrics:', metrics)\n",
    "        fold_metrics.append(metrics)\n",
    "\n",
    "        model_path = os.path.join(save_dir, f'model_fold_{fold}.h5')\n",
    "        model.save(model_path)\n",
    "        print('Saved fold model to', model_path)\n",
    "\n",
    "        if X_train.ndim != 4:\n",
    "            try:\n",
    "                scaler = StandardScaler()\n",
    "                flat = X_train.reshape((X_train.shape[0], -1))\n",
    "                scaler.fit(flat)\n",
    "                scaler_path = os.path.join(save_dir, f'scaler_fold_{fold}.pkl')\n",
    "                joblib.dump(scaler, scaler_path)\n",
    "                print('Saved scaler to', scaler_path)\n",
    "            except Exception as e:\n",
    "                print('Could not save scaler:', e)\n",
    "\n",
    "    avg_metrics = {}\n",
    "    keys = fold_metrics[0].keys()\n",
    "    for k in keys:\n",
    "        vals = [fm[k] for fm in fold_metrics]\n",
    "        avg_metrics[k] = float(np.mean(vals))\n",
    "    print('\\n===== Cross-validation summary =====')\n",
    "    print('Per-fold metrics:', fold_metrics)\n",
    "    print('Average metrics:', avg_metrics)\n",
    "\n",
    "    summary_path = os.path.join(save_dir, 'cv_summary.npy')\n",
    "    np.save(summary_path, {'fold_metrics': fold_metrics, 'avg_metrics': avg_metrics})\n",
    "    print('Saved CV summary to', summary_path)\n",
    "\n",
    "    return fold_metrics, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c392fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training on full data and save\n",
    "def train_final_and_save(X, y, problem_type=None, batch_size=64, epochs=50, save_path='/content/drive/MyDrive/models/final_model.h5'):\n",
    "    if problem_type is None:\n",
    "        problem_type = detect_problem_type(y)\n",
    "    print('Training final model, problem type:', problem_type)\n",
    "\n",
    "    preprocess_train, preprocess_infer = get_preprocessing_fn(X)\n",
    "    X_pre = preprocess_train(X)\n",
    "\n",
    "    if X_pre.ndim == 2:\n",
    "        input_shape = (X_pre.shape[1],)\n",
    "    else:\n",
    "        input_shape = X_pre.shape[1:]\n",
    "\n",
    "    if problem_type == 'classification':\n",
    "        n_classes = len(np.unique(y))\n",
    "    else:\n",
    "        n_classes = None\n",
    "\n",
    "    model = build_model(input_shape=input_shape, problem_type=problem_type, n_classes=n_classes)\n",
    "\n",
    "    ds = make_dataset(X_pre, y, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    cb = [keras.callbacks.EarlyStopping(monitor='loss', patience=6, restore_best_weights=True),]\n",
    "\n",
    "    model.fit(ds, epochs=epochs, callbacks=cb, verbose=2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    model.save(save_path)\n",
    "    print('Saved final model to', save_path)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d777be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example quick-run: mount Drive, set DATA_DIR, run CV and final training\n",
    "# Uncomment and run in Colab.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "DATA_DIR = '/content/AI-Models/Data/preprocessed'  # or '/content/drive/MyDrive/your_data'\n",
    "data = load_data_from_npy(DATA_DIR)\n",
    "if data['X_train'] is not None and data['y_train'] is not None:\n",
    "    X = data['X_train']\n",
    "    y = data['y_train']\n",
    "    print('Using X_train/y_train from preprocessed folder')\n",
    "elif data['X_train'] is None and os.path.exists(os.path.join(DATA_DIR, 'dataset.csv')):\n",
    "    X, y = load_csv(os.path.join(DATA_DIR, 'dataset.csv'), target_col='target')\n",
    "else:\n",
    "    raise RuntimeError('Data not found. Upload files to Colab or mount Drive and set DATA_DIR.')\n",
    "\n",
    "# Run 5-fold CV (adjust epochs/batch_size as needed)\n",
    "fold_metrics, avg_metrics = run_k_fold_cv(X, y, n_splits=5, batch_size=64, epochs=25, save_dir='/content/drive/MyDrive/skyacre_models')\n",
    "\n",
    "# Train final model on all data and save\n",
    "final_path = train_final_and_save(X, y, batch_size=64, epochs=25, save_path='/content/drive/MyDrive/skyacre_models/final_model.h5')\n",
    "\n",
    "print('Done. Models and summaries saved to Google Drive (if mounted).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f2048",
   "metadata": {},
   "source": [
    "**Notes & tips**:\n",
    "- For large image datasets, consider using `tf.keras.preprocessing.image_dataset_from_directory` and augmentations.\n",
    "- Tune `batch_size`, `epochs`, and model depth depending on GPU memory.\n",
    "- To resume training, load saved model `.h5` files with `keras.models.load_model(path)`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
